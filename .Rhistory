arima(c_train.bc, order = c(2,1,2),fixed=c(NA,0,NA,0), method = "ML")
AICc(arima(c_train.bc, order=c(2,1,2), method="ML"))
AICc(arima(c_train.bc, order = c(2,1,2),fixed=c(NA,NA,NA,0), method = "ML"))
# Plot the residuals
fit2 <- arima(c_train.bc, order = c(2,1,2),fixed=c(NA,NA,NA,0), method = "ML")
res2 <- residuals(fit2)
plot.ts(res2, xlab = "")
abline(h=mean(res2),col = "blue")
# Plot the histogram, Q-Q normal plot of the residual par(mfrow=c(1,2))
hist(res2, density = 20, breaks = 20,
col2 = "blue", xlab = "", prob = TRUE, main = "")
m2 <- mean(res2)
std2 <- sqrt(var(res2))
curve(dnorm(x,m2,std2),add = TRUE)
qqnorm(res2,main = "", xlab = "")
qqline(res2, col = "blue")
# Plot the ACF and PACF of the residual
par(mfrow=c(1,2))
acf(res2,lag.max=60, main = "")
pacf(res2,lag.max=60, main = "")
# Perform test on Model 1's residual
shapiro.test(res2)
Box.test(res2,lag = 20, type = c("Box-Pierce"),fitdf = 3)
Box.test(res2,lag = 20, type = c("Ljung-Box"),fitdf = 3)
Box.test(res2^2,lag = 20, type = c("Ljung-Box"),fitdf = 0)
ar(res1,aic=TRUE,order.max=NULL, mehod = c("yule-walker"))
ARIMA <- arima(c_train.bc, order=c(2,1,2), method="ML",fixed=c(NA,NA,NA,0))
pred.demand <- predict(ARIMA, n.ahead = 9)
Upperbound <- pred.demand$pred + 1.96*pred.demand$se
Lowerbound <- pred.demand$pred - 1.96*pred.demand$se
ts.plot(c_train.bc, ylim = c(-2,12))
lines(Upperbound, col="blue")
lines(Lowerbound, col="blue")
points(1:9, y= pred.demand$pred, col="lightpink",cex=1)
#points((length(c_train.bc)+1):(length(c_train.bc)+9), y= pred.demand$pred, col="lightpink",start = c(2022,1,1),cex=1)
#points((length(c_train.bc)+1):(length(c_train.bc)+9), y= c_test, col="lightblue",start = c(2022,1,1),cex=1) # True Value
# sarima.for(c_train.bc, p=2,d=1,q=2,fixed=c(NA,NA,NA,NA,0),n.ahead = 12)
# sarima(c_train.bc, p=2,d=1,q=2)$fit
pred.orig <- exp(pred.demand$pred) # transform back U <- exp(Upperbound)
L <- exp(Lowerbound)
ts.plot(c_train,  ylim = c(1,8), col="black")
lines(U, col="grey", lty="dashed")
fit.2<-arima(c_train, order = c(2,1,2),fixed=c(NA,NA,NA,0), method = "ML")
# Create confidence interval
pred.tr <- predict(fit.2, n.ahead = 12)
pred.tr
U = pred.tr$pred + 1.96*pred.tr$se
L = pred.tr$pred - 1.96*pred.tr$se
# Forecast on original data
plot.ts(c_train, xlim = c(1990,2022), ylim = c(1,10))
lines(U, col = "blue", lty = "dashed")
lines(L, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
# Zoom in graph
plot.ts(c_train, xlim = c(2017,2022))
lines(U, col = "blue", lty = "dashed")
lines(L, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
pred.orig <- exp(pred.demand$pred) # transform back U <- exp(Upperbound)
L <- exp(Lowerbound)
ts.plot(c_train,  ylim = c(1,8), col="black")
lines(U, col="grey", lty="dashed")
lines(L, col="grey", lty="dashed")
points((length(c_train)+1):(length(c_train)+9), pred.orig, col="red", cex=0.5)  #points((length(c_train)+1):(length(c_train)+10), c_test, col="blue", cex=0.5) #True Value
ARIMA <- arima(c_train.bc, order=c(2,1,2), method="ML",fixed=c(NA,NA,NA,0))
pred.demand <- predict(ARIMA, n.ahead = 9)
Upperbound <- pred.demand$pred + 1.96*pred.demand$se
Lowerbound <- pred.demand$pred - 1.96*pred.demand$se
ts.plot(c_train.bc, ylim = c(-2,12))
lines(Upperbound, col="blue")
lines(Lowerbound, col="blue")
points(1:9, y= pred.demand$pred, col="lightpink",cex=1)
#points((length(c_train.bc)+1):(length(c_train.bc)+9), y= pred.demand$pred, col="lightpink",start = c(2022,1,1),cex=1)
#points((length(c_train.bc)+1):(length(c_train.bc)+9), y= c_test, col="lightblue",start = c(2022,1,1),cex=1) # True Value
# sarima.for(c_train.bc, p=2,d=1,q=2,fixed=c(NA,NA,NA,NA,0),n.ahead = 12)
# sarima(c_train.bc, p=2,d=1,q=2)$fit
ARIMA <- arima(c_train.bc, order=c(2,1,2), method="ML",fixed=c(NA,NA,NA,0))
pred.demand <- predict(ARIMA, n.ahead = 9)
Upperbound <- pred.demand$pred + 1.96*pred.demand$se
Lowerbound <- pred.demand$pred - 1.96*pred.demand$se
ts.plot(c_train.bc, ylim = c(-2,12))
lines(Upperbound, col="blue")
lines(Lowerbound, col="blue")
points(2022.1:2022.12, y= pred.demand$pred, col="lightpink",cex=1)
ARIMA <- arima(c_train.bc, order=c(2,1,2), method="ML",fixed=c(NA,NA,NA,0))
pred.demand <- predict(ARIMA, n.ahead = 9)
Upperbound <- pred.demand$pred + 1.96*pred.demand$se
Lowerbound <- pred.demand$pred - 1.96*pred.demand$se
ts.plot(c_train.bc,xlim = c(2021,2022), ylim = c(-2,12))
lines(Upperbound, col="blue")
lines(Lowerbound, col="blue")
points(2022.1:2022.12, y= pred.demand$pred, col="lightpink",cex=1)
ARIMA <- arima(c_train.bc, order=c(2,1,2), method="ML",fixed=c(NA,NA,NA,0))
pred.demand <- predict(ARIMA, n.ahead = 9)
Upperbound <- pred.demand$pred + 1.96*pred.demand$se
Lowerbound <- pred.demand$pred - 1.96*pred.demand$se
ts.plot(c_train.bc,xlim = c(2021.8,2022), ylim = c(-2,12))
lines(Upperbound, col="blue")
lines(Lowerbound, col="blue")
points(2022.1:2022.9, y= pred.demand$pred, col="lightpink",cex=1)
# sarima.for(c_train.bc, p=2,d=1,q=2,fixed=c(NA,NA,NA,NA,0),n.ahead = 12)
# sarima(c_train.bc, p=2,d=1,q=2)$fit
pred.demand$pred
ARIMA <- arima(c_train.bc, order=c(2,1,2), method="ML",fixed=c(NA,NA,NA,0))
pred.demand <- predict(ARIMA, n.ahead = 9)
Upperbound <- pred.demand$pred + 1.96*pred.demand$se
Lowerbound <- pred.demand$pred - 1.96*pred.demand$se
ts.plot(c_train.bc,xlim = c(2021.8,2022), ylim = c(-2,12))
lines(Upperbound, col="blue")
lines(Lowerbound, col="blue")
points(2022.1:2022.9, y= pred.demand$pred, col="lightpink",cex=1)
ARIMA <- arima(c_train.bc, order=c(2,1,2), method="ML",fixed=c(NA,NA,NA,0))
pred.demand <- predict(ARIMA, n.ahead = 9)
Upperbound <- pred.demand$pred + 1.96*pred.demand$se
Lowerbound <- pred.demand$pred - 1.96*pred.demand$se
ts.plot(c_train.bc,xlim = c(2021.8,2022), ylim = c(-2,12))
lines(Upperbound, col="blue")
lines(Lowerbound, col="blue")
points(2022.0:2022.9, y= pred.demand$pred, col="lightpink",cex=1)
# sarima.for(c_train.bc, p=2,d=1,q=2,fixed=c(NA,NA,NA,NA,0),n.ahead = 12)
# sarima(c_train.bc, p=2,d=1,q=2)$fit
length(pred.demand$pred)
knitr::opts_chunk$set(echo = TRUE)
#install.packages("qpcR")
#install.packages("rgl")
setwd("/Users/xuao/Documents/PSTAT296A/pstat296")
library(tsdl)
library(forecast)
library(tidyverse)
library(astsa)
library(MASS)
library(ggplot2)
library(readxl)
library(ggfortify)
library(forecast)
library(GeneCycle)
library(qpcR)
require(TSA)
#source("plot.roots.R")
rm(list = ls())
TR26_21 <- read_excel("/Users/xuao/Documents/PSTAT296A/pstat296/2022-11.11/SBBI Data for Capital Markets (2022_11_11).xlsx", sheet="Processed")
colnames(TR26_21) <- c("Observed Month", "Large Cap TR", "LT Govt TR 20 YR","IT Govt TR 5 YR","LT Corp TR 20 YR","T-Bill TR 30 DAY","US CPI/ INFLATION","LT Govt Yield 20 YEAR","IT Govt Yield 5 YEAR")
TR26_21 <- TR26_21 %>%
filter(!row_number() %in% c(1:5)) %>%
janitor::clean_names()
TR26_21 <- as.data.frame(sapply(TR26_21, as.numeric))
TR26_21 <- TR26_21 %>%
mutate(observed_month = as.Date(observed_month,origin = "1899-12-30"))
TR26_21
Govt_20_Yield <- TR26_21[,c(1,8)]
Govt_20_Yield
Govt_20_Yield$yield_percentage <- round(Govt_20_Yield$lt_govt_yield_20_year*100, digits = 3)
percentage <- ts(Govt_20_Yield$yield_percentage[1:1161], frequency = 12, start = c(1926,1,1))
ts.plot(percentage, type="l")
summary(Govt_20_Yield$yield_percentage)
# Since there is a big difference between the time before index 769 (1990.01) and after that
# So we choose to research in the data after index 709
# Divide it into train and test for nearly 90% and 10%
train <- Govt_20_Yield$yield_percentage[769:1152]
test <- Govt_20_Yield$yield_percentage[1153:1161]
#train set contains data points from 1990.01 to 2021.12, 384 points totally
#test set contains data points from 2022.1 and 2022.9, 9 points totally
train
par(mfrow=c(3, 1))
data <- ts(Govt_20_Yield$yield_percentage[769:1161], frequency = 12, start = c(1990,1,1))
c_train <- ts(train, frequency = 12, start = c(1990,1,1))
c_test <-ts(test,frequency = 12,start = c(2022,1))
ts.plot(data)
ts.plot(c_train)
ts.plot(c_test)
plot.ts(c_train, main="SBBI Interest training data")
fit <- lm(c_train ~ as.numeric(1:length(c_train)))
abline(h=mean(c_train), col="blue")
plot.ts(c_train, main="")
fit <- lm(c_train ~ as.numeric(1:length(c_train)))
abline(fit, col="red")
abline(h=mean(c_train), col="blue")
hist(c_train, main="Histogram of c_train",xlab="date",freq = F)
m <- mean(c_train)
std <- sqrt(var(c_train))
curve(dnorm(x, m, std), col="red", add=TRUE,yaxt = "n")
acf(c_train, lag.max = 40, main = "")
# Box-Cox transformation:
t <- 1:length(c_train)
bcTransform <- boxcox(c_train ~ t, plotit=TRUE)
lambda <- bcTransform$x[which.max(bcTransform$y)]
c_train.bc = (1/lambda)*(c_train^lambda-1)
lambda
plot(c_train.bc)
par(mfrow=c(1,2))
hist(c_train, main="Histogram of c_train",xlab="date")
hist(c_train.bc, main="Histogram of c_train.bc",xlab="date")
# Plot and compare the two:
par(mfrow=c(2,3))
plot.ts(c_train,xlab = "", main = "")
hist(c_train, col = "light blue", xlab = "", main = "")
qqnorm(c_train, main = "", xlab = "")
qqline(c_train, col = "red")
plot.ts(c_train.bc,xlab = "", main = "")
hist(c_train.bc, col = "light blue", xlab = "", main = "")
qqnorm(c_train.bc, main = "", xlab = "")
qqline(c_train.bc, col = "red")
decomp <- decompose(c_train.bc)
plot(decomp)
# Deseasonalize and detrend:
c_train.bc.12 <- diff(c_train.bc, 12)
c_train.bc.12.1 <- diff(c_train.bc.12, 1)
c_train.bc.1 <- diff(c_train.bc, 1)
c_train.bc.1.12 <- diff(c_train.bc.1, 12)
var(c_train.bc)
var(c_train.bc.12);
var(c_train.bc.12.1);
var(c_train.bc.1);
var(c_train.bc.1.12)
hist(c_train.bc.1, col = "light blue", xlab = "", main = "")
# Plot it for De-seasonlized Time Series :
par(mfrow=c(1, 1))
ts.plot(c_train.bc.1, main="De-seasonlized Time Series",
ylab=expression(nabla[1]~Y[t]))
abline(h=mean(c_train.bc.1), lty=2)
acf(c_train.bc.1, lag.max=120, main="ACF")
pacf(c_train.bc.1, lag.max=120, main="PACF")
library(forecast)
auto.arima(c_train.bc.1,ic = c("aicc"),seasonal = F,stepwise = F)
AICc(arima(c_train.bc.1, order=c(0,0,2), method="ML"))
arima(c_train.bc.1, order=c(0,0,2),fixed=c(0,NA,NA), method="ML")
AICc(arima(c_train.bc.1, order=c(0,0,2),fixed=c(0,NA,NA), method="ML"))
polyroot(c(1, 0, -0.1448))
# Plot the residuals
fit1 <- arima(c_train, order = c(0,0,2),
seasonal = list(order = c(0,0,2), period = 12), method = "ML")
res1 <- residuals(fit1)
plot.ts(res1, xlab = "")
abline(h=mean(res1),col = "blue")
# Plot the histogram, Q-Q normal plot of the residual par(mfrow=c(1,2))
hist(res1, density = 20, breaks = 20,
col = "blue", xlab = "", prob = TRUE, main = "")
m1 <- mean(res1)
std1 <- sqrt(var(res1))
curve(dnorm(x,m1,std1),add = TRUE)
qqnorm(res1,main = "", xlab = "")
qqline(res1, col = "blue")
# Plot the ACF and PACF of the residual
par(mfrow=c(1,2))
acf(res1,lag.max=60, main = "")
pacf(res1,lag.max=60, main = "")
# Perform test on Model 1's residual
shapiro.test(res1)
Box.test(res1,lag = 20, type = c("Box-Pierce"),fitdf = 1)
Box.test(res1,lag = 20, type = c("Ljung-Box"),fitdf = 1)
Box.test(res1^2,lag = 20, type = c("Ljung-Box"),fitdf = 0)
ar(res1,aic=TRUE,order.max=NULL, mehod = c("yule-walker"))
fit.1<-arima(c_train, order = c(0,0,2), fixed=c(0,NA,NA), method = "ML")
# Create confidence interval
pred.tr <- predict(fit.1, n.ahead = 12)
U1 = pred.tr$pred + 2*pred.tr$se
L1 = pred.tr$pred - 2*pred.tr$se
# Forecast on original data
plot.ts(c_train, xlim = c(1990,2022), ylim = c(1,10))
lines(U1, col = "blue", lty = "dashed")
lines(L1, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."),
fill = c("red", "blue"), cex = 1.25) # Zoom in graph
plot.ts(c_train, xlim = c(1990,2022), ylim = c(1,max(U1)))
lines(U1, col = "blue", lty = "dashed")
lines(L1, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."),
fill = c("red", "blue"), cex = 1.25) # Forecast with test dataset
plot.ts(milk, xlim = c(1990,2022), ylim = c(1,max(U1)))
lines(U1, col = "blue", lty = "dashed")
lines(L1, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
points(c_test,col = "slategrey", pch = 20)
legend("bottomright", c("Prediction","95% C.I.","c_test"),
fill = c("red", "blue","slategrey"), cex = 1.25)
arima(c_train.bc, order = c(2,1,2), method = "ML")
arima(c_train.bc, order = c(2,1,2),fixed=c(NA,0,NA,0), method = "ML")
AICc(arima(c_train.bc, order=c(2,1,2), method="ML"))
AICc(arima(c_train.bc, order = c(2,1,2),fixed=c(NA,NA,NA,0), method = "ML"))
# Plot the residuals
fit2 <- arima(c_train.bc, order = c(2,1,2),fixed=c(NA,NA,NA,0), method = "ML")
res2 <- residuals(fit2)
plot.ts(res2, xlab = "")
abline(h=mean(res2),col = "blue")
# Plot the histogram, Q-Q normal plot of the residual par(mfrow=c(1,2))
hist(res2, density = 20, breaks = 20,
col2 = "blue", xlab = "", prob = TRUE, main = "")
m2 <- mean(res2)
std2 <- sqrt(var(res2))
curve(dnorm(x,m2,std2),add = TRUE)
qqnorm(res2,main = "", xlab = "")
qqline(res2, col = "blue")
# Plot the ACF and PACF of the residual
par(mfrow=c(1,2))
acf(res2,lag.max=60, main = "")
pacf(res2,lag.max=60, main = "")
# Perform test on Model 1's residual
shapiro.test(res2)
Box.test(res2,lag = 20, type = c("Box-Pierce"),fitdf = 3)
Box.test(res2,lag = 20, type = c("Ljung-Box"),fitdf = 3)
Box.test(res2^2,lag = 20, type = c("Ljung-Box"),fitdf = 0)
ar(res1,aic=TRUE,order.max=NULL, mehod = c("yule-walker"))
ARIMA <- arima(c_train.bc, order=c(2,1,2), method="ML",fixed=c(NA,NA,NA,0))
pred.demand <- predict(ARIMA, n.ahead = 9)
Upperbound <- pred.demand$pred + 1.96*pred.demand$se
Lowerbound <- pred.demand$pred - 1.96*pred.demand$se
ts.plot(c_train.bc,xlim = c(2021.8,2022), ylim = c(-2,12))
lines(Upperbound, col="blue")
lines(Lowerbound, col="blue")
points(2022.0:2022.9, y= pred.demand$pred, col="lightpink",cex=1)
par(mfrow=c(3, 1))
data <- ts(Govt_20_Yield$yield_percentage[769:1161], frequency = 12, start = c(1990,1,1))
c_train <- ts(train, frequency = 12, start = c(1990,1,1))
c_test <-ts(test,frequency = 12,start = c(2022,1))
ts.plot(data)
ts.plot(c_train)
ts.plot(c_test)
par(mfrow=c(3, 1))
data <- ts(Govt_20_Yield$yield_percentage[769:1161], frequency = 12, start = c(1990,1,1))
c_train <- ts(train, frequency = 12, start = c(1990,1,1))
c_test <-ts(test,frequency = 12,start = c(2022,1,1))
ts.plot(data)
ts.plot(c_train)
ts.plot(c_test)
plot.ts(c_train, main="SBBI Interest training data")
fit <- lm(c_train ~ as.numeric(1:length(c_train)))
abline(h=mean(c_train), col="blue")
plot.ts(c_train, main="")
fit <- lm(c_train ~ as.numeric(1:length(c_train)))
abline(fit, col="red")
abline(h=mean(c_train), col="blue")
hist(c_train, main="Histogram of c_train",xlab="date",freq = F)
m <- mean(c_train)
std <- sqrt(var(c_train))
curve(dnorm(x, m, std), col="red", add=TRUE,yaxt = "n")
acf(c_train, lag.max = 40, main = "")
# Box-Cox transformation:
t <- 1:length(c_train)
bcTransform <- boxcox(c_train ~ t, plotit=TRUE)
lambda <- bcTransform$x[which.max(bcTransform$y)]
c_train.bc = (1/lambda)*(c_train^lambda-1)
lambda
plot(c_train.bc)
# Plot and compare the two:
par(mfrow=c(2,3))
plot.ts(c_train,xlab = "", main = "")
hist(c_train, col = "light blue", xlab = "", main = "")
qqnorm(c_train, main = "", xlab = "")
qqline(c_train, col = "red")
plot.ts(c_train.bc,xlab = "", main = "")
hist(c_train.bc, col = "light blue", xlab = "", main = "")
qqnorm(c_train.bc, main = "", xlab = "")
qqline(c_train.bc, col = "red")
arima(c_train.bc, order = c(2,1,2), method = "ML")
arima(c_train.bc, order = c(2,1,2),fixed=c(NA,0,NA,0), method = "ML")
AICc(arima(c_train.bc, order=c(2,1,2), method="ML"))
AICc(arima(c_train.bc, order = c(2,1,2),fixed=c(NA,NA,NA,0), method = "ML"))
# Predict 9 future observations and plot
par(mfrow=c(1, 1))
mypred <- predict(fit2, n.ahead=9)
ts.plot(c(c_train), xlim=c(1, length(c_train) + 10), ylim=c(0, 8))
points((length(c_train) + 1):(length(c_train) + 10), col="red",
(lambda*mypred$pred + 1)^(1/lambda))
# Predict 9 future observations and plot
par(mfrow=c(1, 1))
mypred <- predict(fit2, n.ahead=9)
ts.plot(c(c_train), xlim=c(1, length(c_train) + ), ylim=c(0, 8))
# Predict 9 future observations and plot
par(mfrow=c(1, 1))
mypred <- predict(fit2, n.ahead=9)
ts.plot(c(c_train), xlim=c(1, length(c_train) + 10), ylim=c(0, 8))
points((length(c_train) + 1):(length(c_train) + 10), col="red",
(lambda*mypred$pred + 1)^(1/lambda))
# Predict 9 future observations and plot
par(mfrow=c(1, 1))
mypred <- predict(fit2, n.ahead=9)
ts.plot(c(c_train), xlim=c(1, length(c_train) + 10), ylim=c(0, 8))
lines((length(c_train) + 1):(length(c_train) + 10),
(lambda*mypred$pred + 1.96*mypred$se + 1)^(1/lambda), lty=2)
# Since there is a big difference between the time before index 769 (1990.01) and after that
# So we choose to research in the data after index 709
# Divide it into train and test for nearly 90% and 10%
train <- Govt_20_Yield$yield_percentage[769:1152]
test <- Govt_20_Yield$yield_percentage[1153:1161]
#train set contains data points from 1990.01 to 2021.12, 384 points totally
#test set contains data points from 2022.1 and 2022.9, 9 points totally
test
par(mfrow=c(3, 1))
data <- ts(Govt_20_Yield$yield_percentage[769:1161], frequency = 12, start = c(1990,1,1))
c_train <- ts(train, frequency = 12, start = c(1990,1,1))
c_test <-ts(test,frequency = 12,start = c(2022,1,1))
ts.plot(data)
ts.plot(c_train)
ts.plot(c_test)
c_test
par(mfrow=c(3, 1))
data <- ts(Govt_20_Yield$yield_percentage[769:1161], frequency = 12, start = c(1990,1,1))
c_train <- ts(train, frequency = 12, start = c(1990,1,1))
c_test <-ts(test,frequency = 12,start = c(2022,1,1))
ts.plot(data)
ts.plot(c_train)
ts.plot(c_test)
c_train
pred.tr <- predict(fit2, n.ahead = 9)
U.tr = (lambda*pred.tr$pred + 1.96*pred.tr$se + 1)^(1/lambda)
L.tr = (lambda*pred.tr$pred - 1.96*pred.tr$se + 1)^(1/lambda)
# Forecast on original data
plot.ts(c_train, xlim = c(2015,2022), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."),
fill = c("red", "blue"), cex = 1.25)
# Zoom in graph
par(mfrow=c(1,2))
plot.ts(c_train, xlim = c(2019,2022), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."),
fill = c("red", "blue"), cex = 1.25) # Forecast with test dataset
plot.ts(c_test, xlim = c(1988,1991), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1) points(c_test,col = "slategrey", pch = 20)
pred.tr <- predict(fit2, n.ahead = 9)
U.tr = (lambda*pred.tr$pred + 1.96*pred.tr$se + 1)^(1/lambda)
L.tr = (lambda*pred.tr$pred - 1.96*pred.tr$se + 1)^(1/lambda)
# Forecast on original data
plot.ts(c_train, xlim = c(2015,2022), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."),
fill = c("red", "blue"), cex = 1.25)
# Zoom in graph
par(mfrow=c(1,2))
plot.ts(c_train, xlim = c(2019,2022), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
legend("bottomright", c("Prediction","95% C.I."),
fill = c("red", "blue"), cex = 1.25) # Forecast with test dataset
plot.ts(c_test, xlim = c(1988,1991), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
points(c_test,col = "slategrey", pch = 20)
legend("bottomright", c("Prediction","95% C.I.","c_test"),
fill = c("red", "blue","slategrey"), cex = 1.25)
pred.tr <- predict(fit2, n.ahead = 9)
U.tr = (lambda*pred.tr$pred + 1.96*pred.tr$se + 1)^(1/lambda)
L.tr = (lambda*pred.tr$pred - 1.96*pred.tr$se + 1)^(1/lambda)
# Forecast on original data
plot.ts(c_train, xlim = c(2015,2022), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
# Zoom in graph
par(mfrow=c(1,2))
plot.ts(c_train, xlim = c(2019,2022), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
# Forecast with test dataset
plot.ts(c_test, xlim = c(1988,1991), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
points(c_test,col = "slategrey", pch = 20)
pred.tr <- predict(fit2, n.ahead = 9)
U.tr = (lambda*pred.tr$pred + 1.96*pred.tr$se + 1)^(1/lambda)
L.tr = (lambda*pred.tr$pred - 1.96*pred.tr$se + 1)^(1/lambda)
# Forecast on original data
plot.ts(c_train, xlim = c(2015,2023), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
# Zoom in graph
par(mfrow=c(1,2))
plot.ts(c_train, xlim = c(2019,2023), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
# Forecast with test dataset
plot.ts(c_test, xlim = c(2021,2023), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points(pred.tr$pred,col = "red", pch = 1)
points(c_test,col = "slategrey", pch = 20)
pred.tr <- predict(fit2, n.ahead = 9)
U.tr = (lambda*pred.tr$pred + 1.96*pred.tr$se + 1)^(1/lambda)
L.tr = (lambda*pred.tr$pred - 1.96*pred.tr$se + 1)^(1/lambda)
# Forecast on original data
plot.ts(c_train, xlim = c(2015,2023), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points((lambda*pred.tr$pred + 1)^(1/lambda),col = "red", pch = 1)
# Zoom in graph
par(mfrow=c(1,2))
plot.ts(c_train, xlim = c(2019,2023), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points((lambda*pred.tr$pred + 1)^(1/lambda),col = "red", pch = 1)
# Forecast with test dataset
plot.ts(c_test, xlim = c(2021,2023), ylim = c(0,8))
lines(U.tr, col = "blue", lty = "dashed")
lines(L.tr, col = "blue", lty = "dashed")
points((lambda*pred.tr$pred + 1)^(1/lambda),col = "red", pch = 1)
points(c_test,col = "slategrey", pch = 20)
