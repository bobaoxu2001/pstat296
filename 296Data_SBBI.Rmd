---
title: "296Data_SBBI"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# SBBI Raw Data for Capital Markets (2022_10_21)
```{r}
#install.packages("qpcR")
#install.packages("rgl")
```

```{r,echo=FALSE, result = 'hide',fig.show='hide',message=FALSE}
setwd("/Users/xuao/Documents/PSTAT296A/pstat296")
library(tsdl) 
library(forecast) 
library(tidyverse) 
library(astsa)
library(MASS) 
library(ggplot2) 
library(readxl)
library(ggfortify) 
library(forecast) 
library(GeneCycle) 
library(qpcR) 
require(TSA) 
#source("plot.roots.R")
```

```{r}
rm(list = ls())
```

# Import SBBI Raw Data for Capital Markets (2022_11_11)
# Renewed data for LT Govt Yield (20yr) from Jan-1926 to Sep-2022
```{r}
TR26_21 <- read_excel("/Users/xuao/Documents/PSTAT296A/pstat296/2022-11.11/SBBI Data for Capital Markets (2022_11_11).xlsx", sheet="Processed")
colnames(TR26_21) <- c("Observed Month", "Large Cap TR", "LT Govt TR 20 YR","IT Govt TR 5 YR","LT Corp TR 20 YR","T-Bill TR 30 DAY","US CPI/ INFLATION","LT Govt Yield 20 YEAR","IT Govt Yield 5 YEAR")
TR26_21 <- TR26_21 %>%
  filter(!row_number() %in% c(1:5)) %>%
  janitor::clean_names()
TR26_21 <- as.data.frame(sapply(TR26_21, as.numeric))
TR26_21 <- TR26_21 %>%
  mutate(observed_month = as.Date(observed_month,origin = "1899-12-30")) 
TR26_21
```

```{r}
Govt_20_Yield <- TR26_21[,c(1,8)]
Govt_20_Yield 
Govt_20_Yield$yield_percentage <- round(Govt_20_Yield$lt_govt_yield_20_year*100, digits = 3)
percentage <- ts(Govt_20_Yield$yield_percentage[1:1161], frequency = 12, start = c(1926,1,1))
ts.plot(percentage, type="l")
```

```{r}
summary(Govt_20_Yield$yield_percentage)
```

```{r}
# Since there is a big difference between the time before index 769 (1990.01) and after that
# So we choose to research in the data after index 709
# Divide it into train and test for nearly 90% and 10%
train <- Govt_20_Yield$yield_percentage[769:1149]
test <- Govt_20_Yield$yield_percentage[1153:1161]
#train set contains data points from 1990.01 to 2021.12, 381 points totally
#test set contains data points from 2022.1 and 2022.9, 9 points totally
```

```{r}
par(mfrow=c(3, 1))
data <- ts(Govt_20_Yield$yield_percentage[769:1161], frequency = 12, start = c(1990,1,1))
c_train <- ts(train, frequency = 12, start = c(1990,1,1)) 
c_test <-ts(test,frequency = 12,start = c(2022,1))
ts.plot(data)
ts.plot(c_train)
ts.plot(c_test)
```
```{r}
plot.ts(c_train, main="SBBI Interest training data") 
fit <- lm(c_train ~ as.numeric(1:length(c_train)))
abline(h=mean(c_train), col="blue")
```

```{r}
hist(c_train, main="Histogram of c_train",xlab="date")
```
```{r}
acf(c_train, lag.max = 40, main = "")
```
To make the data stationary (variance/seasonality/trend),
,we use the Box-Cox transformation firstly
```{r}
# Box-Cox transformation:
t <- 1:length(c_train)
bcTransform <- boxcox(c_train ~ t, plotit=TRUE)
lambda <- bcTransform$x[which.max(bcTransform$y)]
c_train.bc = (1/lambda)*(c_train^lambda-1)
lambda
plot(c_train.bc)
```
```{r}
hist(c_train.bc, main="Histogram of c_train.bc",xlab="date")
```

```{r}
# Plot and compare the two:
par(mfrow=c(2,3))
plot.ts(c_train,xlab = "", main = "")
hist(c_train, col = "light blue", xlab = "", main = "") 
qqnorm(c_train, main = "", xlab = "")
qqline(c_train, col = "red")
plot.ts(c_train.bc,xlab = "", main = "")
hist(c_train.bc, col = "light blue", xlab = "", main = "")
qqnorm(c_train.bc, main = "", xlab = "")
qqline(c_train.bc, col = "red")
```
```{r}
decomp <- decompose(c_train.bc)
plot(decomp)
```
From the comparisons, we could find that Box-Cox transformation makes it more normal
Since the Box-Cox plot does not contain neither has 0 nor 1 in the confidence interval for $\lambda$, we choose
$\lambda \approx$ 0.6262626 that maximizes the log-likelihood.

```{r}
# Deseasonalize and detrend:
c_train.bc.12 <- diff(c_train.bc, 12)
c_train.bc.12.1 <- diff(c_train.bc.12, 1)
c_train.bc.1 <- diff(c_train.bc, 1)
c_train.bc.1.12 <- diff(c_train.bc.1, 12)
var(c_train.bc)
var(c_train.bc.12);
var(c_train.bc.12.1);
var(c_train.bc.1);
var(c_train.bc.1.12)
```
```{r}
hist(c_train.bc.1, col = "light blue", xlab = "", main = "") 
```


```{r}
# Plot it for De-seasonlized Time Series :
par(mfrow=c(1, 1))
ts.plot(c_train.bc.1, main="De-seasonlized Time Series",
ylab=expression(nabla[1]~Y[t]))
abline(h=mean(c_train.bc.1), lty=2)
```
### Model Identification

```{r}
acf(c_train.bc.1, lag.max=120, main="ACF")
pacf(c_train.bc.1, lag.max=120, main="PACF")
```
ACF outside confidence intervals: Lags 2, may be 
PACF outside confidence intervals: Lags 1, 2, 3, 4
d = 1; 
q = 2; 
p = 2;


```{r}
library(forecast)
auto.arima(c_train.bc.1,ic = c("aicc"),seasonal = F,stepwise = F)
```

```{r}
arima(c_train.bc.1,order=c(2,1,2), method="ML")
```
```{r}
AICc(arima(c_train.bc.1, order=c(0,0,2), method="ML")) 
```

### Model 1: ARIMA(0,0,2)
```{r}
arima(c_train.bc.1, order=c(0,0,2),fixed=c(0,NA,NA), method="ML")
```
```{r}
AICc(arima(c_train.bc.1, order=c(0,0,2),fixed=c(0,NA,NA), method="ML"))
```

$U_t = (1 – 0.1488B^2)Z_t -0.0145$

```{r}
polyroot(c(1, 0, -0.1448))
```

```{r}
# Plot the residuals
fit1 <- arima(c_train, order = c(0,0,2),
seasonal = list(order = c(0,0,2), period = 12), method = "ML") 
res1 <- residuals(fit1)
plot.ts(res1, xlab = "")
abline(h=mean(res1),col = "blue")
# Plot the histogram, Q-Q normal plot of the residual par(mfrow=c(1,2))
hist(res1, density = 20, breaks = 20,
col = "blue", xlab = "", prob = TRUE, main = "") 
m1 <- mean(res1)
std1 <- sqrt(var(res1)) 
curve(dnorm(x,m1,std1),add = TRUE)
qqnorm(res1,main = "", xlab = "") 
qqline(res1, col = "blue")
# Plot the ACF and PACF of the residual
par(mfrow=c(1,2)) 
acf(res1,lag.max=60, main = "") 
pacf(res1,lag.max=60, main = "")
```
```{r}
# Perform test on Model 1's residual
shapiro.test(res1)
Box.test(res1,lag = 20, type = c("Box-Pierce"),fitdf = 1) 
Box.test(res1,lag = 20, type = c("Ljung-Box"),fitdf = 1) 
Box.test(res1^2,lag = 20, type = c("Ljung-Box"),fitdf = 0) 
ar(res1,aic=TRUE,order.max=NULL, mehod = c("yule-walker"))
```

```{r}
fit.1<-arima(c_train, order = c(0,0,2), fixed=c(0,NA,NA), method = "ML")
# Create confidence interval
pred.tr <- predict(fit.1, n.ahead = 12)
U1 = pred.tr$pred + 2*pred.tr$se
L1 = pred.tr$pred - 2*pred.tr$se
# Forecast on original data
plot.ts(c_train, xlim = c(1990,2022), ylim = c(1,10)) 
lines(U1, col = "blue", lty = "dashed")
lines(L1, col = "blue", lty = "dashed") 
points(pred.tr$pred,col = "red", pch = 1) 
legend("bottomright", c("Prediction","95% C.I."),


fill = c("red", "blue"), cex = 1.25) # Zoom in graph
plot.ts(c_train, xlim = c(1990,2022), ylim = c(1,max(U1)))
lines(U1, col = "blue", lty = "dashed")
lines(L1, col = "blue", lty = "dashed") 
points(pred.tr$pred,col = "red", pch = 1) 
legend("bottomright", c("Prediction","95% C.I."),

       
fill = c("red", "blue"), cex = 1.25) # Forecast with test dataset
plot.ts(milk, xlim = c(1990,2022), ylim = c(1,max(U1))) 
lines(U1, col = "blue", lty = "dashed")
lines(L1, col = "blue", lty = "dashed") 
points(pred.tr$pred,col = "red", pch = 1)
points(c_test,col = "slategrey", pch = 20) 
legend("bottomright", c("Prediction","95% C.I.","c_test"),
fill = c("red", "blue","slategrey"), cex = 1.25)
```
#### Model 2: ARIMA(2,1,2)
```{r}
arima(c_train.bc, order = c(2,1,2), method = "ML")
```

```{r}
arima(c_train.bc, order = c(2,1,2),fixed=c(NA,0,NA,0), method = "ML")
```

$ (1+B)U_t = (1 – 0.9677B)Z_t$ 
```{r}
AICc(arima(c_train.bc.1, order=c(2,1,2), method="ML")) 
```
```{r}
AICc(arima(c_train.bc, order = c(2,1,2),fixed=c(NA,0,NA,0), method = "ML"))
```


```{r}
# Plot the residuals
fit2 <- arima(c_train.bc, order = c(2,1,2),fixed=c(NA,0,NA,0), method = "ML") 
res2 <- residuals(fit2)
plot.ts(res2, xlab = "")
abline(h=mean(res2),col = "blue")
# Plot the histogram, Q-Q normal plot of the residual par(mfrow=c(1,2))
hist(res2, density = 20, breaks = 20,
col2 = "blue", xlab = "", prob = TRUE, main = "") 
m2 <- mean(res2)
std2 <- sqrt(var(res2)) 
curve(dnorm(x,m2,std2),add = TRUE)
qqnorm(res2,main = "", xlab = "") 
qqline(res2, col = "blue")
# Plot the ACF and PACF of the residual
par(mfrow=c(1,2)) 
acf(res2,lag.max=60, main = "") 
pacf(res2,lag.max=60, main = "")
```

```{r}
# Perform test on Model 1's residual
shapiro.test(res2)
Box.test(res2,lag = 20, type = c("Box-Pierce"),fitdf = 2) 
Box.test(res2,lag = 20, type = c("Ljung-Box"),fitdf = 2) 
Box.test(res2^2,lag = 20, type = c("Ljung-Box"),fitdf = 0) 
ar(res1,aic=TRUE,order.max=NULL, mehod = c("yule-walker"))
```

```{r}
fit.1<-arima(c_train, order = c(0,1,1),
seasonal = list(order = c(0,1,4), period = 12), method = "ML")
# Create confidence interval
pred.tr <- predict(fit.1, n.ahead = 12)
U = pred.tr$pred + 2*pred.tr$se
L = pred.tr$pred - 2*pred.tr$se
# Forecast on original data
plot.ts(c_train, xlim = c(1962,1976), ylim = c(500,1000)) 
lines(U, col = "blue", lty = "dashed")
lines(L, col = "blue", lty = "dashed") 
points(pred.tr$pred,col = "red", pch = 1) 
legend("bottomright", c("Prediction","95% C.I."),


fill = c("red", "blue"), cex = 1.25) # Zoom in graph
plot.ts(c_train, xlim = c(1973,1976), ylim = c(700,max(U)))
lines(U, col = "blue", lty = "dashed")
lines(L, col = "blue", lty = "dashed") 
points(pred.tr$pred,col = "red", pch = 1) 
legend("bottomright", c("Prediction","95% C.I."),

       
fill = c("red", "blue"), cex = 1.25) # Forecast with test dataset
plot.ts(milk, xlim = c(1973,1976), ylim = c(700,max(U))) 
lines(U, col = "blue", lty = "dashed")
lines(L, col = "blue", lty = "dashed") 
points(pred.tr$pred,col = "red", pch = 1)
points(c_test,col = "slategrey", pch = 20) 
legend("bottomright", c("Prediction","95% C.I.","c_test"),
fill = c("red", "blue","slategrey"), cex = 1.25)
```

```{r}
# Forecast with test dataset
plot.ts(data, xlim = c(2010,2025), ylim = c(0,7))
lines(U.tr, col = "blue", lty = "dashed") 
lines(L.tr, col = "blue", lty = "dashed") 
points(pred.tr$pred,col = "red", pch = 1) 
points(c_test,col = "slategrey", pch = 20) 
```




